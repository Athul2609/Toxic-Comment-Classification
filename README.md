# Toxic Comment Classification Model

This repository contains code and resources for building a toxic comment classification model capable of identifying six different types of toxicities: toxic, severe toxic, threat, obscene, insult, and identity hate. The dataset used for this project is sourced from the Kaggle competition hosted by Jigsaw. You can find the dataset and competition details [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).

## Goal

The primary goal of this project is to develop a highly accurate toxic comment classification model that can achieve a position in the top 1% of solutions on the Kaggle leaderboard. We aim to create a model that not only identifies toxic comments but also provides fine-grained classification for the different types of toxicities mentioned above.

## Project Status

This project is a work in progress. We are actively working on data preprocessing, feature engineering, model selection, and hyperparameter tuning to achieve the best possible results. As we make progress, we will update this README and the code in the repository accordingly.